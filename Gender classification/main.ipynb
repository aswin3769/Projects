{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d975b9-659e-4417-9cdc-93c17877d50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 14:33:09.331996: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-13 14:33:09.645341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744554789.762575    2259 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744554789.800496    2259 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744554789.986023    2259 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744554789.986071    2259 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744554789.986072    2259 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744554789.986074    2259 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-13 14:33:10.006687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import splitfolders\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,InputLayer\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d595c8a3-4815-4a1c-9838-e4b603eb35e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 27167 files [10:09, 44.58 files/s]\n"
     ]
    }
   ],
   "source": [
    "splitfolders.ratio('faces','dataset',seed = 42,ratio=(.8,.1,.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b991465e-a5fc-42a0-8da9-1e91dae36b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'dataset/train'\n",
    "test_data = 'dataset/test'\n",
    "val_data ='dataset/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6d4de8-5b27-4491-9585-916cac4d1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013f7870-6454-47a7-90a7-6817bd095486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21733 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744554824.504403    2259 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_data,\n",
    "    image_size=(img_size,img_size),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_split=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "527788a4-d1d9-41a2-8f7a-345f3aa09ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_category = train_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62dbd9f5-2436-4c31-8696-b244abff3184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['man', 'woman']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1339967-98a1-4ce6-b04b-788795f31dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2719 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data,\n",
    "    image_size=(img_size,img_size),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    validation_split=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c61b5e9-5d65-47f6-8d2d-969820b9fc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2715 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_data,\n",
    "    image_size=(img_size,img_size),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    validation_split=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937ccb99-3537-44e9-b9ce-9765670901a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(120,reshuffle_each_iteration=True).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.shuffle(120,reshuffle_each_iteration=True).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10e21b5f-f45c-4191-a20c-e29acfa072f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    Conv2D(16 , (3,3) , padding = 'same' , activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(32 , (3,3) , padding = 'same' , activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(64 , (3,3) , padding = 'same' , activation = 'relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128 , activation = 'relu'),\n",
    "    \n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    Dense(1 , activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "997f98a7-e2d5-4e82-b566-3259262df716",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam' , loss = BinaryCrossentropy() , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab79513-0b52-486a-a511-9afcfd05a635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744554842.909878    2359 service.cc:152] XLA service 0x7f15c401e990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744554842.910882    2359 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-04-13 14:34:03.127658: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1744554843.548504    2359 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-04-13 14:34:05.285120: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1049', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-04-13 14:34:05.682449: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1049', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-04-13 14:34:10.656810: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng57{k2=8,k13=1,k14=1,k18=1,k23=0} for conv %cudnn-conv-bw-filter.4 = (f32[32,16,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,16,90,90]{3,2,1,0} %bitcast.5528, f32[32,32,90,90]{3,2,1,0} %bitcast.5582), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/sequential_1/conv2d_1_2/convolution/Conv2DBackpropFilter\" source_file=\"/home/unix/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-13 14:34:10.740643: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.924476502s\n",
      "Trying algorithm eng57{k2=8,k13=1,k14=1,k18=1,k23=0} for conv %cudnn-conv-bw-filter.4 = (f32[32,16,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,16,90,90]{3,2,1,0} %bitcast.5528, f32[32,32,90,90]{3,2,1,0} %bitcast.5582), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/sequential_1/conv2d_1_2/convolution/Conv2DBackpropFilter\" source_file=\"/home/unix/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/680\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:16:22\u001b[0m 17s/step - accuracy: 0.6875 - loss: 0.6800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744554852.467926    2359 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 64ms/step - accuracy: 0.7467 - loss: 0.5098 - val_accuracy: 0.8158 - val_loss: 0.3927 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.8745 - loss: 0.2921 - val_accuracy: 0.8980 - val_loss: 0.2521 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m679/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9168 - loss: 0.2077 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 14:35:41.530994: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 12441856 bytes after encountering the first element of size 12441856 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.9168 - loss: 0.2077 - val_accuracy: 0.9215 - val_loss: 0.2071 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9385 - loss: 0.1510 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 14:36:24.241274: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 12441856 bytes after encountering the first element of size 12441856 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 51ms/step - accuracy: 0.9385 - loss: 0.1510 - val_accuracy: 0.9101 - val_loss: 0.2399 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.9604 - loss: 0.0980 - val_accuracy: 0.9145 - val_loss: 0.2720 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m679/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9775 - loss: 0.0662 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 14:37:06.454517: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 12441856 bytes after encountering the first element of size 12441856 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m680/680\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.9775 - loss: 0.0661 - val_accuracy: 0.9050 - val_loss: 0.3307 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f169e587820>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset , validation_data = val_dataset , epochs = 20 , verbose = 1 , callbacks = [ tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 3,\n",
    "    restore_best_weights = True\n",
    "),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',  \n",
    "    factor=0.6,  \n",
    "    patience=4, \n",
    "    min_lr=1e-6  \n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "237b3d48-3abd-4bbe-81da-40b68b416509",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'man.jpg'\n",
    "image = tf.keras.utils.load_img(image,target_size=(img_size,img_size))\n",
    "img_bat = tf.expand_dims(image,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ff4054d-1c77-40bd-97af-0b0ca4a99335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def men_or_woman(value):\n",
    "    if value > 0.5 :\n",
    "        return 1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "395689e3-9ecb-4cdf-add8-3af44e62d3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(img_bat)\n",
    "predict_value = predict[0]\n",
    "score = men_or_woman(predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9506aa4-1efd-48d1-ba88-90ce9e8abe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE GENDER OF PERSON IN IMAGE IS man WITH ACCURACY OF [0.04307611]\n"
     ]
    }
   ],
   "source": [
    "print(f'THE GENDER OF PERSON IN IMAGE IS {data_category[score]} WITH ACCURACY OF {predict_value*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "770983f3-9edc-478e-89f1-224e229a053c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45d219c8-7f54-4979-911d-5f3d54fb7fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 119ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00627946],\n",
       "       [0.23200566],\n",
       "       [0.00827636],\n",
       "       ...,\n",
       "       [0.0160919 ],\n",
       "       [0.9996146 ],\n",
       "       [0.53042984]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f208f00f-263a-4ed4-91f5-3941844e21b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i , (image,label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m9\u001b[39m)):\n\u001b[1;32m      2\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(image[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for i , (image,label) in enumerate(test_dataset.take(9)):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(image[0]/255)\n",
    "    print(label.numpy()[0],model.predict(image)[0][0])\n",
    "    plt.title(str(men_or_woman(label.numpy()[0]))+ ':' + str(men_or_woman(model.predict(image)[0][0])))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae0cdd0c-9d66-4e59-b51e-3ab75f038959",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gender_classify.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953a8c4-14b4-4236-95d7-0040e02f7769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
